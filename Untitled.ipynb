{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05ee9586-9727-4d40-bdfc-95512f873ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import librosa\n",
    "import cv2\n",
    "from transformers import AutoTokenizer, AutoModel \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import Counter\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "class MELDDataPreprocessor:\n",
    "    \"\"\"Handles data preprocessing and balancing for the MELD dataset\"\"\"\n",
    "    \n",
    "    def __init__(self, random_state: int = 42):\n",
    "        self.random_state = random_state\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        \n",
    "    def load_meld_data(self, data_path: str) -> pd.DataFrame:\n",
    "        \"\"\"Load MELD dataset and perform initial preprocessing\"\"\"\n",
    "        data_path=\"data_set\"\n",
    "        df = pd.read_csv(data_path)\n",
    "        df['sentiment_encoded'] = self.label_encoder.fit_transform(df['sentiment'])\n",
    "        return df\n",
    "    \n",
    "    def random_undersample(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Perform random undersampling to balance the dataset\"\"\"\n",
    "        class_counts = Counter(df['sentiment_encoded'])\n",
    "        min_class_count = min(class_counts.values())\n",
    "        \n",
    "        balanced_dfs = []\n",
    "        for class_label in class_counts.keys():\n",
    "            class_df = df[df['sentiment_encoded'] == class_label]\n",
    "            if len(class_df) > min_class_count:\n",
    "                class_df = class_df.sample(n=min_class_count, \n",
    "                                         random_state=self.random_state)\n",
    "            balanced_dfs.append(class_df)\n",
    "        \n",
    "        return pd.concat(balanced_dfs, axis=0).reset_index(drop=True)\n",
    "\n",
    "class MELDFeatureExtractor:\n",
    "    \"\"\"Extracts features from multimodal MELD data\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "    def extract_audio_features(self, audio_path: str) -> np.ndarray:\n",
    "        \"\"\"Extract MFCC and other audio features\"\"\"\n",
    "        y, sr = librosa.load(audio_path)\n",
    "        features = []\n",
    "        \n",
    "        # MFCC features\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "        features.append(mfcc)\n",
    "        \n",
    "        # Spectral features\n",
    "        spectral_centroids = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "        features.append(spectral_centroids)\n",
    "        \n",
    "        # Chromagram\n",
    "        chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "        features.append(chroma)\n",
    "        \n",
    "        return np.concatenate(features, axis=0)\n",
    "    \n",
    "    def extract_video_features(self, video_path: str) -> np.ndarray:\n",
    "        \"\"\"Extract visual features including facial expressions\"\"\"\n",
    "        face_cascade = cv2.CascadeClassifier(\n",
    "            cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'\n",
    "        )\n",
    "        \n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames_features = []\n",
    "        \n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "                \n",
    "            # Detect faces\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "            \n",
    "            for (x, y, w, h) in faces:\n",
    "                face_roi = frame[y:y+h, x:x+w]\n",
    "                # Extract face embeddings or features here\n",
    "                # For now, we'll use basic statistics of the face region\n",
    "                face_features = cv2.resize(face_roi, (64, 64)).flatten()\n",
    "                frames_features.append(face_features)\n",
    "        \n",
    "        cap.release()\n",
    "        return np.array(frames_features)\n",
    "\n",
    "class MultimodalTransformer(nn.Module):\n",
    "    \"\"\"Enhanced Transformer model for multimodal sentiment analysis\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes: int, dropout_rate: float = 0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Text encoder\n",
    "        self.text_encoder = AutoModel.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "        # Audio encoder with attention\n",
    "        self.audio_encoder = nn.Sequential(\n",
    "            nn.Conv1d(20, 64, kernel_size=3),  # Increased input channels for more features\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Conv1d(64, 128, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.AdaptiveAvgPool1d(1)\n",
    "        )\n",
    "        \n",
    "        # Video encoder with 3D convolutions\n",
    "        self.video_encoder = nn.Sequential(\n",
    "            nn.Conv3d(3, 64, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm3d(64),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Conv3d(64, 128, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm3d(128),\n",
    "            nn.AdaptiveAvgPool3d(1)\n",
    "        )\n",
    "        \n",
    "        # Cross-modal attention\n",
    "        self.cross_attention = nn.MultiheadAttention(\n",
    "            embed_dim=128,\n",
    "            num_heads=8,\n",
    "            dropout=dropout_rate\n",
    "        )\n",
    "        \n",
    "        # Final classification\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(768 + 256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, text_ids, text_mask, audio_features, video_features):\n",
    "        # Process text\n",
    "        text_output = self.text_encoder(text_ids, attention_mask=text_mask)\n",
    "        text_embeddings = text_output.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        # Process audio and video\n",
    "        audio_embeddings = self.audio_encoder(audio_features)\n",
    "        video_embeddings = self.video_encoder(video_features)\n",
    "        \n",
    "        # Cross-modal attention between audio and video\n",
    "        av_features, _ = self.cross_attention(\n",
    "            audio_embeddings, \n",
    "            video_embeddings, \n",
    "            video_embeddings\n",
    "        )\n",
    "        \n",
    "        # Concatenate all features\n",
    "        combined = torch.cat([text_embeddings, av_features], dim=1)\n",
    "        \n",
    "        return self.classifier(combined)\n",
    "\n",
    "class SentimentTrainer:\n",
    "    \"\"\"Handles model training and evaluation\"\"\"\n",
    "    \n",
    "    def __init__(self, model: nn.Module, device: str = 'cuda'):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        \n",
    "        # Use weighted cross entropy for class imbalance\n",
    "        class_weights = torch.FloatTensor([1.0, 1.0, 1.0]).to(device)  # Adjust based on class distribution\n",
    "        self.criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        self.optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "        \n",
    "    def train_epoch(self, train_loader: DataLoader) -> float:\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            # Move batch to device\n",
    "            text_ids = batch['text_ids'].to(self.device)\n",
    "            text_mask = batch['text_mask'].to(self.device)\n",
    "            audio_features = batch['audio_features'].to(self.device)\n",
    "            video_features = batch['video_features'].to(self.device)\n",
    "            labels = batch['label'].to(self.device)\n",
    "            \n",
    "            outputs = self.model(text_ids, text_mask, audio_features, video_features)\n",
    "            loss = self.criterion(outputs, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        return total_loss / len(train_loader)\n",
    "    \n",
    "    def evaluate(self, val_loader: DataLoader) -> Dict[str, float]:\n",
    "        self.model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        val_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                text_ids = batch['text_ids'].to(self.device)\n",
    "                text_mask = batch['text_mask'].to(self.device)\n",
    "                audio_features = batch['audio_features'].to(self.device)\n",
    "                video_features = batch['video_features'].to(self.device)\n",
    "                labels = batch['label'].to(self.device)\n",
    "                \n",
    "                outputs = self.model(text_ids, text_mask, audio_features, video_features)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                \n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        return {\n",
    "            'accuracy': correct / total,\n",
    "            'val_loss': val_loss / len(val_loader)\n",
    "        }\n",
    "\n",
    "#print(\"Accuracy:\", accuracy)\n",
    "# print(\"Validation loss:\", val_loss)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7c2bc4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Sr No.', 'Utterance', 'Speaker', 'Emotion', 'Sentiment', 'Dialogue_ID',\n",
      "       'Utterance_ID', 'Season', 'Episode', 'StartTime', 'EndTime'],\n",
      "      dtype='object')\n",
      "   Sr No.                                          Utterance          Speaker  \\\n",
      "0       1  also I was the point person on my company’s tr...         Chandler   \n",
      "1       2                   You must’ve had your hands full.  The Interviewer   \n",
      "2       3                            That I did. That I did.         Chandler   \n",
      "3       4      So let’s talk a little bit about your duties.  The Interviewer   \n",
      "4       5                             My duties?  All right.         Chandler   \n",
      "\n",
      "    Emotion Sentiment  Dialogue_ID  Utterance_ID  Season  Episode  \\\n",
      "0   neutral   neutral            0             0       8       21   \n",
      "1   neutral   neutral            0             1       8       21   \n",
      "2   neutral   neutral            0             2       8       21   \n",
      "3   neutral   neutral            0             3       8       21   \n",
      "4  surprise  positive            0             4       8       21   \n",
      "\n",
      "      StartTime       EndTime  \n",
      "0  00:16:16,059  00:16:21,731  \n",
      "1  00:16:21,940  00:16:23,442  \n",
      "2  00:16:23,442  00:16:26,389  \n",
      "3  00:16:26,820  00:16:29,572  \n",
      "4  00:16:34,452  00:16:40,917  \n"
     ]
    }
   ],
   "source": [
    "# Print column names\n",
    "print(train_df.columns)\n",
    "\n",
    "# Print first few rows of data\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0ae7920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e21e43ca4c24049b2d806e9285b4d5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Multimodal_Senti_Ana\\senti_ana\\lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ACER\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02cbe83d099542e39949130efad5caa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "130439be38844ff1af20043c7f2400a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 9989\n",
      "Emotion classes: {'neutral': 0, 'surprise': 1, 'fear': 2, 'sadness': 3, 'joy': 4, 'disgust': 5, 'anger': 6}\n",
      "Sentiment classes: {'neutral': 0, 'positive': 1, 'negative': 2}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "class SentimentEmotionDataset(Dataset):\n",
    "    def __init__(self, dataframe, max_length=128):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Create label mappings\n",
    "        self.emotion_map = {label: idx for idx, label in enumerate(dataframe['Emotion'].unique())}\n",
    "        self.sentiment_map = {label: idx for idx, label in enumerate(dataframe['Sentiment'].unique())}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        text = str(row['Utterance'])\n",
    "        emotion = self.emotion_map[row['Emotion']]\n",
    "        sentiment = self.sentiment_map[row['Sentiment']]\n",
    "        \n",
    "        # Tokenize the text\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'emotion': torch.tensor(emotion, dtype=torch.long),\n",
    "            'sentiment': torch.tensor(sentiment, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Load your data\n",
    "train_df = pd.read_csv('data_set/train_sent_emo.csv')\n",
    "val_df = pd.read_csv('data_set/dev_sent_emo.csv')  # if you have this\n",
    "\n",
    "# Create dataset instances\n",
    "train_dataset = SentimentEmotionDataset(train_df)\n",
    "val_dataset = SentimentEmotionDataset(val_df)  # if you have validation data\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=16,  # adjust based on your GPU memory\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Print some information about the data\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "print(f\"Emotion classes: {train_dataset.emotion_map}\")\n",
    "print(f\"Sentiment classes: {train_dataset.sentiment_map}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fc8bd4e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AccuracyOptimizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[43mAccuracyOptimizer\u001b[49m(model)\n\u001b[0;32m      2\u001b[0m history \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mtrain_with_validation(train_loader, val_loader)\n\u001b[0;32m      3\u001b[0m test_metrics \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mevaluate_test_set(test_loader)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'AccuracyOptimizer' is not defined"
     ]
    }
   ],
   "source": [
    "optimizer = AccuracyOptimizer(model)\n",
    "history = optimizer.train_with_validation(train_loader, val_loader)\n",
    "test_metrics = optimizer.evaluate_test_set(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d793634b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# 1. Modified model for text-only sentiment and emotion classification\n",
    "class SentimentEmotionModel(nn.Module):\n",
    "    def __init__(self, num_emotions, num_sentiments):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Text encoder (BERT)\n",
    "        self.text_encoder = AutoModel.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "        # Separate classification heads for emotion and sentiment\n",
    "        self.emotion_classifier = nn.Sequential(\n",
    "            nn.Linear(768, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, num_emotions)\n",
    "        )\n",
    "        \n",
    "        self.sentiment_classifier = nn.Sequential(\n",
    "            nn.Linear(768, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, num_sentiments)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Process text\n",
    "        outputs = self.text_encoder(input_ids, attention_mask=attention_mask)\n",
    "        embeddings = outputs.last_hidden_state[:, 0, :]  # Get CLS token\n",
    "        \n",
    "        # Generate predictions\n",
    "        emotion_output = self.emotion_classifier(embeddings)\n",
    "        sentiment_output = self.sentiment_classifier(embeddings)\n",
    "        \n",
    "        return emotion_output, sentiment_output\n",
    "\n",
    "# 2. Dataset class for your data\n",
    "class SentimentEmotionDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=128):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Create label mappings\n",
    "        self.emotion_map = {label: idx for idx, label in enumerate(dataframe['Emotion'].unique())}\n",
    "        self.sentiment_map = {label: idx for idx, label in enumerate(dataframe['Sentiment'].unique())}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        text = str(row['Utterance'])\n",
    "        \n",
    "        # Tokenize text\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'emotion': torch.tensor(self.emotion_map[row['Emotion']]),\n",
    "            'sentiment': torch.tensor(self.sentiment_map[row['Sentiment']])\n",
    "        }\n",
    "\n",
    "# 3. Modified AccuracyOptimizer for dual task learning\n",
    "class AccuracyOptimizer:\n",
    "    def __init__(self, model, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        \n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=2e-5,\n",
    "            weight_decay=0.01\n",
    "        )\n",
    "        \n",
    "        self.scheduler = ReduceLROnPlateau(\n",
    "            self.optimizer,\n",
    "            mode='max',\n",
    "            factor=0.5,\n",
    "            patience=2,\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def train_epoch(self, train_loader):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            # Move data to device\n",
    "            input_ids = batch['input_ids'].to(self.device)\n",
    "            attention_mask = batch['attention_mask'].to(self.device)\n",
    "            emotion_labels = batch['emotion'].to(self.device)\n",
    "            sentiment_labels = batch['sentiment'].to(self.device)\n",
    "            \n",
    "            # Forward pass\n",
    "            emotion_outputs, sentiment_outputs = self.model(input_ids, attention_mask)\n",
    "            \n",
    "            # Calculate losses\n",
    "            emotion_loss = self.criterion(emotion_outputs, emotion_labels)\n",
    "            sentiment_loss = self.criterion(sentiment_outputs, sentiment_labels)\n",
    "            \n",
    "            # Combined loss\n",
    "            loss = emotion_loss + sentiment_loss\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        return total_loss / len(train_loader)\n",
    "\n",
    "    def validate(self, val_loader):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        emotion_preds, sentiment_preds = [], []\n",
    "        emotion_labels, sentiment_labels = [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                batch_emotion_labels = batch['emotion'].to(self.device)\n",
    "                batch_sentiment_labels = batch['sentiment'].to(self.device)\n",
    "                \n",
    "                emotion_outputs, sentiment_outputs = self.model(input_ids, attention_mask)\n",
    "                \n",
    "                # Calculate losses\n",
    "                emotion_loss = self.criterion(emotion_outputs, batch_emotion_labels)\n",
    "                sentiment_loss = self.criterion(sentiment_outputs, batch_sentiment_labels)\n",
    "                loss = emotion_loss + sentiment_loss\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                # Get predictions\n",
    "                _, emotion_pred = torch.max(emotion_outputs, 1)\n",
    "                _, sentiment_pred = torch.max(sentiment_outputs, 1)\n",
    "                \n",
    "                emotion_preds.extend(emotion_pred.cpu().numpy())\n",
    "                sentiment_preds.extend(sentiment_pred.cpu().numpy())\n",
    "                emotion_labels.extend(batch_emotion_labels.cpu().numpy())\n",
    "                sentiment_labels.extend(batch_sentiment_labels.cpu().numpy())\n",
    "        \n",
    "        # Calculate metrics\n",
    "        emotion_f1 = f1_score(emotion_labels, emotion_preds, average='weighted')\n",
    "        sentiment_f1 = f1_score(sentiment_labels, sentiment_preds, average='weighted')\n",
    "        \n",
    "        return {\n",
    "            'val_loss': total_loss / len(val_loader),\n",
    "            'emotion_f1': emotion_f1,\n",
    "            'sentiment_f1': sentiment_f1\n",
    "        }\n",
    "\n",
    "# 4. Training setup\n",
    "def train_model(train_df, val_df, num_epochs=10):\n",
    "    # Initialize tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = SentimentEmotionDataset(train_df, tokenizer)\n",
    "    val_dataset = SentimentEmotionDataset(val_df, tokenizer)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "    \n",
    "    # Initialize model\n",
    "    num_emotions = len(train_dataset.emotion_map)\n",
    "    num_sentiments = len(train_dataset.sentiment_map)\n",
    "    model = SentimentEmotionModel(num_emotions, num_sentiments)\n",
    "    \n",
    "    # Initialize optimizer\n",
    "    optimizer = AccuracyOptimizer(model)\n",
    "    \n",
    "    # Training loop\n",
    "    best_f1 = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        \n",
    "        # Train for one epoch\n",
    "        train_loss = optimizer.train_epoch(train_loader)\n",
    "        print(f\"Training Loss: {train_loss:.4f}\")\n",
    "        \n",
    "        # Evaluate\n",
    "        metrics = optimizer.validate(val_loader)\n",
    "        print(f\"Validation Loss: {metrics['val_loss']:.4f}\")\n",
    "        print(f\"Emotion F1: {metrics['emotion_f1']:.4f}\")\n",
    "        print(f\"Sentiment F1: {metrics['sentiment_f1']:.4f}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Save best model\n",
    "        avg_f1 = (metrics['emotion_f1'] + metrics['sentiment_f1']) / 2\n",
    "        if avg_f1 > best_f1:\n",
    "            best_f1 = avg_f1\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fef0f74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in d:\\multimodal_senti_ana\\senti_ana\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: transformers in d:\\multimodal_senti_ana\\senti_ana\\lib\\site-packages (4.47.1)\n",
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.post12.tar.gz (2.6 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × python setup.py egg_info did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [15 lines of output]\n",
      "      The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
      "      rather than 'sklearn' for pip commands.\n",
      "      \n",
      "      Here is how to fix this error in the main use cases:\n",
      "      - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
      "      - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
      "        (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
      "      - if the 'sklearn' package is used by one of your dependencies,\n",
      "        it would be great if you take some time to track which package uses\n",
      "        'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
      "      - as a last resort, set the environment variable\n",
      "        SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
      "      \n",
      "      More information is available at\n",
      "      https://github.com/scikit-learn/sklearn-pypi-package\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "× Encountered error while generating package metadata.\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    }
   ],
   "source": [
    "! pip install torch transformers sklearn numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b849665",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2f025b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import cv2\n",
    "from transformers import AutoTokenizer\n",
    "from typing import Dict, List, Tuple\n",
    "import os\n",
    "\n",
    "class MELDDataset(Dataset):\n",
    "    def __init__(self, data_path: str, video_dir: str, audio_dir: str, max_length: int = 128):\n",
    "        \"\"\"\n",
    "        Initialize MELD dataset\n",
    "        \n",
    "        Args:\n",
    "            data_path: Path to the CSV file containing MELD annotations\n",
    "            video_dir: Directory containing video files\n",
    "            audio_dir: Directory containing audio files\n",
    "            max_length: Maximum length for text tokenization\n",
    "        \"\"\"\n",
    "        self.data = pd.read_csv(data_set/train_sent_emo.csv)\n",
    "        self.video_dir = video_dir\n",
    "        self.audio_dir = audio_dir\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Initialize tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "        # Map sentiment labels to integers\n",
    "        self.sentiment_map = {\n",
    "            'neutral': 0,\n",
    "            'positive': 1,\n",
    "            'negative': 2\n",
    "        }\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        row = self.data.iloc[idx]\n",
    "        \n",
    "        # Process text\n",
    "        text_encoding = self._process_text(row['Utterance'])\n",
    "        \n",
    "        # Process audio\n",
    "        audio_features = self._process_audio(os.path.join(self.audio_dir, row['Audio_ID']))\n",
    "        \n",
    "        # Process video\n",
    "        video_features = self._process_video(os.path.join(self.video_dir, row['Video_ID']))\n",
    "        \n",
    "        # Get label\n",
    "        label = self.sentiment_map[row['Sentiment']]\n",
    "        \n",
    "        return {\n",
    "            'text_ids': text_encoding['input_ids'],\n",
    "            'text_mask': text_encoding['attention_mask'],\n",
    "            'audio_features': audio_features,\n",
    "            'video_features': video_features,\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "    \n",
    "    def _process_text(self, text: str) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Tokenize text and convert to tensor\"\"\"\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze()\n",
    "        }\n",
    "    \n",
    "    def _process_audio(self, audio_path: str) -> torch.Tensor:\n",
    "        \"\"\"Extract audio features using librosa\"\"\"\n",
    "        try:\n",
    "            # Load audio file\n",
    "            y, sr = librosa.load(audio_path, duration=10)  # Load first 10 seconds\n",
    "            \n",
    "            # Extract features\n",
    "            mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "            mfcc_delta = librosa.feature.delta(mfcc)\n",
    "            mfcc_delta2 = librosa.feature.delta(mfcc, order=2)\n",
    "            \n",
    "            # Combine features\n",
    "            features = np.concatenate([mfcc, mfcc_delta, mfcc_delta2], axis=0)\n",
    "            \n",
    "            # Pad or truncate to fixed length\n",
    "            target_length = 1000\n",
    "            if features.shape[1] < target_length:\n",
    "                features = np.pad(features, ((0, 0), (0, target_length - features.shape[1])))\n",
    "            else:\n",
    "                features = features[:, :target_length]\n",
    "            \n",
    "            return torch.FloatTensor(features)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing audio file {audio_path}: {str(e)}\")\n",
    "            return torch.zeros((39, 1000))  # Return zero tensor with expected shape\n",
    "    \n",
    "    def _process_video(self, video_path: str) -> torch.Tensor:\n",
    "        \"\"\"Extract video features using OpenCV\"\"\"\n",
    "        try:\n",
    "            cap = cv2.VideoCapture(video_path)\n",
    "            frames = []\n",
    "            max_frames = 30  # Extract features from first 30 frames\n",
    "            \n",
    "            while len(frames) < max_frames and cap.isOpened():\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                    \n",
    "                # Resize frame\n",
    "                frame = cv2.resize(frame, (112, 112))\n",
    "                \n",
    "                # Convert to RGB\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                \n",
    "                # Normalize\n",
    "                frame = frame / 255.0\n",
    "                \n",
    "                frames.append(frame)\n",
    "            \n",
    "            cap.release()\n",
    "            \n",
    "            # Pad if necessary\n",
    "            while len(frames) < max_frames:\n",
    "                frames.append(np.zeros((112, 112, 3)))\n",
    "            \n",
    "            # Convert to tensor\n",
    "            frames_tensor = torch.FloatTensor(np.array(frames))\n",
    "            \n",
    "            # Reshape to [C, T, H, W]\n",
    "            frames_tensor = frames_tensor.permute(3, 0, 1, 2)\n",
    "            \n",
    "            return frames_tensor\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing video file {video_path}: {str(e)}\")\n",
    "            return torch.zeros((3, 30, 112, 112))  # Return zero tensor with expected shape\n",
    "\n",
    "def create_data_loaders(\n",
    "    data_path: str,\n",
    "    video_dir: str,\n",
    "    audio_dir: str,\n",
    "    batch_size: int = 32,\n",
    "    train_ratio: float = 0.7,\n",
    "    val_ratio: float = 0.15,\n",
    "    num_workers: int = 4\n",
    ") -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
    "    \"\"\"\n",
    "    Create train, validation, and test data loaders\n",
    "    \n",
    "    Args:\n",
    "        data_path: Path to the CSV file containing MELD annotations\n",
    "        video_dir: Directory containing video files\n",
    "        audio_dir: Directory containing audio files\n",
    "        batch_size: Batch size for data loaders\n",
    "        train_ratio: Proportion of data to use for training\n",
    "        val_ratio: Proportion of data to use for validation\n",
    "        num_workers: Number of worker processes for data loading\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (train_loader, val_loader, test_loader)\n",
    "    \"\"\"\n",
    "    # Create dataset\n",
    "    dataset = MELDDataset(data_path, video_dir, audio_dir)\n",
    "    \n",
    "    # Calculate split sizes\n",
    "    total_size = len(dataset)\n",
    "    train_size = int(train_ratio * total_size)\n",
    "    val_size = int(val_ratio * total_size)\n",
    "    test_size = total_size - train_size - val_size\n",
    "    \n",
    "    # Split dataset\n",
    "    train_dataset, val_dataset, test_dataset = random_split(\n",
    "        dataset, \n",
    "        [train_size, val_size, test_size],\n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49465512",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m AccuracyOptimizer(model)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m history \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mtrain_with_validation(\u001b[43mtrain_loader\u001b[49m, val_loader)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Get test metrics\u001b[39;00m\n\u001b[0;32m     13\u001b[0m test_metrics \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mevaluate_test_set(test_loader)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create your data loaders first (train_loader, val_loader, test_loader)\n",
    "# Then:\n",
    "\n",
    "# Initialize model and optimizer\n",
    "model = MultimodalSentimentModel(num_classes=3)     \n",
    "#model = MultimodalTransformer(num_classes=3) \n",
    "optimizer = AccuracyOptimizer(model)\n",
    "\n",
    "# Train the model\n",
    "history = optimizer.train_with_validation(train_loader, val_loader)\n",
    "\n",
    "# Get test metrics\n",
    "test_metrics = optimizer.evaluate_test_set(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06dd41e8-8485-43e9-a277-430bf180ad8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipywidgets\n",
      "  Using cached ipywidgets-8.1.5-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in d:\\multimodal_senti_ana\\senti_ana\\lib\\site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in d:\\multimodal_senti_ana\\senti_ana\\lib\\site-packages (from ipywidgets) (8.18.1)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in d:\\multimodal_senti_ana\\senti_ana\\lib\\site-packages (from ipywidgets) (5.14.3)\n",
      "Collecting widgetsnbextension~=4.0.12 (from ipywidgets)\n",
      "  Using cached widgetsnbextension-4.0.13-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab-widgets~=3.0.12 (from ipywidgets)\n",
      "  Using cached jupyterlab_widgets-3.0.13-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: decorator in d:\\multimodal_senti_ana\\senti_ana\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in d:\\multimodal_senti_ana\\senti_ana\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in d:\\multimodal_senti_ana\\senti_ana\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in d:\\multimodal_senti_ana\\senti_ana\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.48)\n",
      "Requirement already satisfied: pygments>=2.4.0 in d:\\multimodal_senti_ana\\senti_ana\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.18.0)\n",
      "Requirement already satisfied: stack-data in d:\\multimodal_senti_ana\\senti_ana\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: typing-extensions in d:\\multimodal_senti_ana\\senti_ana\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (4.12.2)\n",
      "Requirement already satisfied: exceptiongroup in d:\\multimodal_senti_ana\\senti_ana\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (1.2.2)\n",
      "Requirement already satisfied: colorama in d:\\multimodal_senti_ana\\senti_ana\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in d:\\multimodal_senti_ana\\senti_ana\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: wcwidth in d:\\multimodal_senti_ana\\senti_ana\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in d:\\multimodal_senti_ana\\senti_ana\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in d:\\multimodal_senti_ana\\senti_ana\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure_eval in d:\\multimodal_senti_ana\\senti_ana\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Using cached ipywidgets-8.1.5-py3-none-any.whl (139 kB)\n",
      "Using cached jupyterlab_widgets-3.0.13-py3-none-any.whl (214 kB)\n",
      "Using cached widgetsnbextension-4.0.13-py3-none-any.whl (2.3 MB)\n",
      "Installing collected packages: widgetsnbextension, jupyterlab-widgets, ipywidgets\n",
      "Successfully installed ipywidgets-8.1.5 jupyterlab-widgets-3.0.13 widgetsnbextension-4.0.13\n"
     ]
    }
   ],
   "source": [
    "!pip install ipywidgets\n",
    "#conda install -c conda-forge ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b10789a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "class AccuracyOptimizer:\n",
    "    def __init__(self, model: nn.Module, device: str = 'cuda'):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        \n",
    "        # Initialize optimizer with weight decay\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=2e-5,\n",
    "            weight_decay=0.01  # L2 regularization\n",
    "        )\n",
    "        \n",
    "        # Learning rate scheduler\n",
    "        self.scheduler = ReduceLROnPlateau(\n",
    "            self.optimizer,\n",
    "            mode='max',\n",
    "            factor=0.5,\n",
    "            patience=2,\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        # Focal Loss for better handling of class imbalance\n",
    "        self.criterion = FocalLoss(gamma=2.0)\n",
    "        \n",
    "    def train_with_validation(\n",
    "        self,\n",
    "        train_loader: torch.utils.data.DataLoader,\n",
    "        val_loader: torch.utils.data.DataLoader,\n",
    "        num_epochs: int = 10\n",
    "    ) -> Dict[str, List[float]]:\n",
    "        history = {\n",
    "            'train_loss': [],\n",
    "            'val_loss': [],\n",
    "            'val_accuracy': [],\n",
    "            'val_f1': []\n",
    "        }\n",
    "        \n",
    "        best_f1 = 0.0\n",
    "        patience_counter = 0\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            # Training phase\n",
    "            train_loss = self._train_epoch(train_loader)\n",
    "            history['train_loss'].append(train_loss)\n",
    "            \n",
    "            # Validation phase\n",
    "            metrics = self._validate(val_loader)\n",
    "            history['val_loss'].append(metrics['val_loss'])\n",
    "            history['val_accuracy'].append(metrics['accuracy'])\n",
    "            history['val_f1'].append(metrics['f1_score'])\n",
    "            \n",
    "            # Learning rate scheduling\n",
    "            self.scheduler.step(metrics['f1_score'])\n",
    "            \n",
    "            # Early stopping check\n",
    "            if metrics['f1_score'] > best_f1:\n",
    "                best_f1 = metrics['f1_score']\n",
    "                patience_counter = 0\n",
    "                # Save best model\n",
    "                torch.save(self.model.state_dict(), 'best_model.pth')\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= 5:  # Early stopping patience\n",
    "                    print(\"Early stopping triggered\")\n",
    "                    break\n",
    "                    \n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "            print(f\"Train Loss: {train_loss:.4f}\")\n",
    "            print(f\"Val Loss: {metrics['val_loss']:.4f}\")\n",
    "            print(f\"Val Accuracy: {metrics['accuracy']:.4f}\")\n",
    "            print(f\"Val F1 Score: {metrics['f1_score']:.4f}\")\n",
    "            print(\"--------------------\")\n",
    "            \n",
    "        return history\n",
    "    \n",
    "    def _train_epoch(self, train_loader) -> float:\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            outputs = self.model(\n",
    "                batch['text_ids'].to(self.device),\n",
    "                batch['text_mask'].to(self.device),\n",
    "                batch['audio_features'].to(self.device),\n",
    "                batch['video_features'].to(self.device)\n",
    "            )\n",
    "            \n",
    "            loss = self.criterion(outputs, batch['label'].to(self.device))\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            self.optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        return total_loss / len(train_loader)\n",
    "    \n",
    "    def _validate(self, val_loader) -> Dict[str, float]:\n",
    "        self.model.eval()\n",
    "        val_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                outputs = self.model(\n",
    "                    batch['text_ids'].to(self.device),\n",
    "                    batch['text_mask'].to(self.device),\n",
    "                    batch['audio_features'].to(self.device),\n",
    "                    batch['video_features'].to(self.device)\n",
    "                )\n",
    "                \n",
    "                labels = batch['label'].to(self.device)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = np.mean(np.array(all_preds) == np.array(all_labels))\n",
    "        f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "        \n",
    "        return {\n",
    "            'val_loss': val_loss / len(val_loader),\n",
    "            'accuracy': accuracy,\n",
    "            'f1_score': f1\n",
    "        }\n",
    "    \n",
    "    def evaluate_test_set(self, test_loader) -> Dict[str, float]:\n",
    "        self.model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                outputs = self.model(\n",
    "                    batch['text_ids'].to(self.device),\n",
    "                    batch['text_mask'].to(self.device),\n",
    "                    batch['audio_features'].to(self.device),\n",
    "                    batch['video_features'].to(self.device)\n",
    "                )\n",
    "                \n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(batch['label'].cpu().numpy())\n",
    "        \n",
    "        # Generate detailed metrics\n",
    "        conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "        class_report = classification_report(all_labels, all_preds, output_dict=True)\n",
    "        \n",
    "        return {\n",
    "            'confusion_matrix': conf_matrix,\n",
    "            'classification_report': class_report,\n",
    "            'accuracy': class_report['accuracy'],\n",
    "            'macro_f1': class_report['macro avg']['f1-score']\n",
    "        }\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = nn.CrossEntropyLoss(reduction='none')(inputs, targets)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma * ce_loss).mean()\n",
    "        return focal_loss\n",
    "\n",
    "def get_optimization_suggestions(metrics: Dict[str, float]) -> List[str]:\n",
    "    suggestions = []\n",
    "    \n",
    "    if metrics['accuracy'] < 0.7:\n",
    "        suggestions.extend([\n",
    "            \"Consider increasing model complexity\",\n",
    "            \"Try different learning rates\",\n",
    "            \"Add more regularization\"\n",
    "        ])\n",
    "    \n",
    "    if metrics['macro_f1'] < 0.6:\n",
    "        suggestions.extend([\n",
    "            \"Check class distribution\",\n",
    "            \"Adjust class weights\",\n",
    "            \"Try data augmentation\"\n",
    "        ])\n",
    "    \n",
    "    return suggestions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
